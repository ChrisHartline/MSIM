# -*- coding: utf-8 -*-
"""HDC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10DUgpvMcIokg752Ua0ecjC_7pcI-L36D
"""

# ============================================================================
# Day 5: Hyperdimensional Computing for Anomaly Detection (HyperDUM)
# MSIM815 Final Project - Safety Layer
# ============================================================================

"""
PURPOSE: Implement HyperDUM for real-time anomaly detection
OUTPUTS:
  - Trained HyperDUM model
  - Anomaly detection performance
  - Safety monitoring capability
"""

# ============================================================================
# SETUP
# ============================================================================

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report
import time
import json
import os

BASE_DIR = '/content/drive/MyDrive/MSIM815'
DATA_DIR = f'{BASE_DIR}/data/ffnn_training'
RESULTS_DIR = f'{BASE_DIR}/results/day5'

os.makedirs(RESULTS_DIR, exist_ok=True)

print("âœ… Setup complete")

# ============================================================================
# HYPERDUM CLASS
# ============================================================================

class HyperDUM:
    """
    Hyperdimensional Computing for Uncertainty Monitoring

    Uses hypervectors (D=10,000 dimensions) to encode position patterns
    and detect anomalies through similarity scoring.
    """

    def __init__(self, D=10000, levels=100):
        """
        Initialize HyperDUM

        Args:
            D: Hypervector dimensionality (default 10,000)
            levels: Number of encoding levels per dimension (default 100)
        """
        self.D = D
        self.levels = levels
        self.prototype = None
        self.stats = {}

        # Generate random base hypervectors for each dimension
        np.random.seed(42)
        self.base_hvs = {
            'x': np.random.choice([-1, 1], size=(levels, D)),
            'y': np.random.choice([-1, 1], size=(levels, D)),
            'z': np.random.choice([-1, 1], size=(levels, D))
        }

    def encode(self, positions):
        """
        Encode 3D positions into hypervectors

        Args:
            positions: (N, 3) array of [x, y, z] positions

        Returns:
            encoded: (N, D) hypervectors
        """
        N = len(positions)
        encoded = np.zeros((N, self.D))

        # Get ranges for normalization
        if not hasattr(self, 'ranges'):
            self.ranges = {
                'x': (positions[:, 0].min(), positions[:, 0].max()),
                'y': (positions[:, 1].min(), positions[:, 1].max()),
                'z': (positions[:, 2].min(), positions[:, 2].max())
            }

        for i, pos in enumerate(positions):
            # Discretize each dimension
            x_idx = self._discretize(pos[0], 'x')
            y_idx = self._discretize(pos[1], 'y')
            z_idx = self._discretize(pos[2], 'z')

            # Bind (element-wise multiply) the three hypervectors
            encoded[i] = (self.base_hvs['x'][x_idx] *
                         self.base_hvs['y'][y_idx] *
                         self.base_hvs['z'][z_idx])

        return encoded

    def _discretize(self, value, dim):
        """Discretize continuous value to level index"""
        min_val, max_val = self.ranges[dim]
        normalized = (value - min_val) / (max_val - min_val + 1e-10)
        level = int(normalized * (self.levels - 1))
        return np.clip(level, 0, self.levels - 1)

    def train(self, normal_positions):
        """
        Train by creating prototype from normal samples

        Args:
            normal_positions: (N, 3) array of normal [x, y, z] positions
        """
        print("ðŸ”„ Encoding normal samples...")
        encoded = self.encode(normal_positions)

        print("ðŸ”„ Creating prototype...")
        # Prototype = sign of sum (voting)
        self.prototype = np.sign(np.sum(encoded, axis=0))

        # Calculate statistics
        similarities = self._similarity(encoded, self.prototype)
        self.stats = {
            'mean': float(np.mean(similarities)),
            'std': float(np.std(similarities)),
            'min': float(np.min(similarities)),
            'max': float(np.max(similarities))
        }

        print(f"âœ… Training complete")
        print(f"   Prototype similarity: {self.stats['mean']:.4f} Â± {self.stats['std']:.4f}")

    def _similarity(self, hvs, prototype):
        """
        Calculate cosine similarity to prototype

        Args:
            hvs: (N, D) hypervectors
            prototype: (D,) prototype hypervector

        Returns:
            similarities: (N,) similarity scores
        """
        # Cosine similarity = dot product / D (since vectors are Â±1)
        return hvs @ prototype / self.D

    def predict(self, positions, threshold=None):
        """
        Predict anomalies

        Args:
            positions: (N, 3) array of [x, y, z] positions
            threshold: Similarity threshold (auto if None)

        Returns:
            anomalies: (N,) boolean array (True = anomaly)
            scores: (N,) similarity scores
        """
        encoded = self.encode(positions)
        scores = self._similarity(encoded, self.prototype)

        if threshold is None:
            # Use 25th percentile as threshold
            threshold = np.percentile(scores, 25)

        anomalies = scores < threshold

        return anomalies, scores, threshold

# ============================================================================
# LOAD DATA
# ============================================================================

print("\nðŸ“‚ Loading data...")

# Load test data (corrected positions from Day 4)
test_positions = pd.read_csv(f'{DATA_DIR}/test/ekf_outputs.csv').values
test_corrections = pd.read_csv(f'{DATA_DIR}/test/corrections.csv').values
test_gt = test_positions + test_corrections

print(f"   Test samples: {len(test_positions):,}")

# ============================================================================
# GENERATE ANOMALOUS SAMPLES
# ============================================================================

print("\nðŸ”„ Generating anomalous samples...")

np.random.seed(42)

# Normal samples
n_normal = len(test_positions)
normal_positions = test_gt.copy()

# Anomalous samples (various failure modes)
n_anomalous = int(0.1 * n_normal)  # 10% anomalies

anomalous_positions = []
anomaly_types = []

# Type 1: GPS spoofing (large offset)
n_spoof = n_anomalous // 3
spoof_samples = test_gt[np.random.choice(n_normal, n_spoof, replace=False)]
spoof_samples += np.random.randn(n_spoof, 3) * 50  # Large error
anomalous_positions.append(spoof_samples)
anomaly_types.extend(['spoofing'] * n_spoof)

# Type 2: Sensor dropout (extreme values)
n_dropout = n_anomalous // 3
dropout_samples = test_gt[np.random.choice(n_normal, n_dropout, replace=False)]
dropout_samples[:, np.random.randint(0, 3, n_dropout)] = 999  # Extreme value
anomalous_positions.append(dropout_samples)
anomaly_types.extend(['dropout'] * n_dropout)

# Type 3: Multipath (large variance)
n_multipath = n_anomalous - n_spoof - n_dropout
multipath_samples = test_gt[np.random.choice(n_normal, n_multipath, replace=False)]
multipath_samples += np.random.randn(n_multipath, 3) * 30  # High variance
anomalous_positions.append(multipath_samples)
anomaly_types.extend(['multipath'] * n_multipath)

anomalous_positions = np.vstack(anomalous_positions)

print(f"   Normal: {n_normal:,}")
print(f"   Anomalous: {n_anomalous:,}")
print(f"     - Spoofing: {n_spoof}")
print(f"     - Dropout: {n_dropout}")
print(f"     - Multipath: {n_multipath}")

# Combine and create labels
all_positions = np.vstack([normal_positions, anomalous_positions])
all_labels = np.concatenate([
    np.zeros(n_normal, dtype=int),  # 0 = normal
    np.ones(n_anomalous, dtype=int)  # 1 = anomaly
])

# Shuffle
shuffle_idx = np.random.permutation(len(all_positions))
all_positions = all_positions[shuffle_idx]
all_labels = all_labels[shuffle_idx]

# ============================================================================
# TRAIN HYPERDUM
# ============================================================================

print("\nðŸ”„ Training HyperDUM...")

hyperdum = HyperDUM(D=10000, levels=100)

train_start = time.time()
hyperdum.train(normal_positions)
train_time = time.time() - train_start

print(f"   Training time: {train_time:.3f}s")

# ============================================================================
# EVALUATE
# ============================================================================

print("\nðŸ“Š Evaluating anomaly detection...")

predict_start = time.time()
predictions, scores, threshold = hyperdum.predict(all_positions)
predict_time = time.time() - predict_start

print(f"   Prediction time: {predict_time:.3f}s")
print(f"   Samples/sec: {len(all_positions)/predict_time:,.0f}")
print(f"   Threshold: {threshold:.4f}")

# Calculate metrics
accuracy = accuracy_score(all_labels, predictions)
precision = precision_score(all_labels, predictions)
recall = recall_score(all_labels, predictions)
f1 = f1_score(all_labels, predictions)

print(f"\n{'='*70}")
print("ANOMALY DETECTION PERFORMANCE")
print(f"{'='*70}")
print(f"\n   Accuracy:  {accuracy*100:.2f}%")
print(f"   Precision: {precision*100:.2f}%")
print(f"   Recall:    {recall*100:.2f}%")
print(f"   F1-Score:  {f1*100:.2f}%")

# Confusion matrix
cm = confusion_matrix(all_labels, predictions)
print(f"\nConfusion Matrix:")
print(f"   TN: {cm[0,0]:,}  FP: {cm[0,1]:,}")
print(f"   FN: {cm[1,0]:,}  TP: {cm[1,1]:,}")

# ============================================================================
# VISUALIZE
# ============================================================================

print("\nðŸ“Š Creating visualizations...")

fig = plt.figure(figsize=(16, 10))

# Plot 1: Similarity score distribution
ax1 = plt.subplot(2, 3, 1)
normal_scores = scores[all_labels == 0]
anomaly_scores = scores[all_labels == 1]

ax1.hist(normal_scores, bins=50, alpha=0.6, color='green', label='Normal', density=True)
ax1.hist(anomaly_scores, bins=50, alpha=0.6, color='red', label='Anomaly', density=True)
ax1.axvline(threshold, color='black', linestyle='--', linewidth=2, label='Threshold')
ax1.set_xlabel('Similarity Score', fontweight='bold')
ax1.set_ylabel('Density', fontweight='bold')
ax1.set_title('Similarity Score Distribution', fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: ROC-like curve
ax2 = plt.subplot(2, 3, 2)
thresholds_range = np.linspace(scores.min(), scores.max(), 100)
recalls = []
precisions = []

for thresh in thresholds_range:
    pred = scores < thresh
    if pred.sum() > 0:
        recalls.append(recall_score(all_labels, pred))
        precisions.append(precision_score(all_labels, pred))
    else:
        recalls.append(0)
        precisions.append(0)

ax2.plot(recalls, precisions, 'b-', linewidth=2)
ax2.scatter([recall], [precision], color='red', s=100, marker='o',
           label=f'Operating Point\n(R={recall:.2f}, P={precision:.2f})', zorder=5)
ax2.set_xlabel('Recall', fontweight='bold')
ax2.set_ylabel('Precision', fontweight='bold')
ax2.set_title('Precision-Recall Curve', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Confusion matrix heatmap
ax3 = plt.subplot(2, 3, 3)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3,
           cbar_kws={'label': 'Count'})
ax3.set_xlabel('Predicted', fontweight='bold')
ax3.set_ylabel('Actual', fontweight='bold')
ax3.set_title('Confusion Matrix', fontweight='bold')
ax3.set_xticklabels(['Normal', 'Anomaly'])
ax3.set_yticklabels(['Normal', 'Anomaly'])

# Plot 4: Performance metrics bar chart
ax4 = plt.subplot(2, 3, 4)
metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
metrics_values = [accuracy, precision, recall, f1]
colors = ['blue', 'green', 'orange', 'purple']

bars = ax4.bar(metrics_names, metrics_values, color=colors, alpha=0.7,
              edgecolor='black', linewidth=2)
for bar, val in zip(bars, metrics_values):
    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
            f'{val*100:.1f}%', ha='center', fontweight='bold')

ax4.set_ylabel('Score', fontweight='bold')
ax4.set_title('Detection Performance Metrics', fontweight='bold')
ax4.set_ylim([0, 1.1])
ax4.grid(True, alpha=0.3, axis='y')

# Plot 5: Processing time
ax5 = plt.subplot(2, 3, 5)
stages = ['Training\n(Normal)', 'Inference\n(Per Sample)']
times = [train_time, predict_time / len(all_positions) * 1000]  # ms per sample
colors = ['blue', 'green']

bars = ax5.bar(stages, times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
bars[0].set_label(f'{train_time:.3f}s')
bars[1].set_label(f'{times[1]:.3f}ms')

ax5.set_ylabel('Time', fontweight='bold')
ax5.set_title('Processing Time', fontweight='bold')
ax5.grid(True, alpha=0.3, axis='y')

for bar, t, unit in zip(bars, [train_time, times[1]], ['s', 'ms']):
    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.05,
            f'{t:.3f}{unit}', ha='center', fontweight='bold')

# Plot 6: Summary
ax6 = plt.subplot(2, 3, 6)
ax6.axis('off')

summary = f"""
DAY 5 SUMMARY
{'='*45}

HyperDUM Configuration:
  Dimensionality:  {hyperdum.D:,}
  Encoding levels: {hyperdum.levels}
  Binding:         Element-wise multiply
  Prototype:       Sign voting

Dataset:
  Normal samples:  {n_normal:,}
  Anomalous:       {n_anomalous:,}
    - Spoofing:    {n_spoof}
    - Dropout:     {n_dropout}
    - Multipath:   {n_multipath}

Performance:
  Accuracy:        {accuracy*100:.2f}%
  Precision:       {precision*100:.2f}%
  Recall:          {recall*100:.2f}%
  F1-Score:        {f1*100:.2f}%

Efficiency:
  Training:        {train_time:.3f}s
  Inference:       {predict_time/len(all_positions)*1000:.3f}ms/sample
  Throughput:      {len(all_positions)/predict_time:,.0f} samples/sec

Threshold:         {threshold:.4f}
"""

ax6.text(0.1, 0.5, summary, fontsize=10, family='monospace', verticalalignment='center')

plt.suptitle('Day 5: HyperDUM Anomaly Detection', fontsize=15, fontweight='bold')
plt.tight_layout()

viz_path = f'{RESULTS_DIR}/day5_hyperdum_results.png'
plt.savefig(viz_path, dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ… Visualization saved: {viz_path}")

# ============================================================================
# SAVE RESULTS
# ============================================================================

print("\nðŸ’¾ Saving results...")

# Save model
model_data = {
    'D': hyperdum.D,
    'levels': hyperdum.levels,
    'prototype': hyperdum.prototype.tolist(),
    'ranges': hyperdum.ranges,
    'stats': hyperdum.stats,
    'threshold': float(threshold)
}

model_path = f'{RESULTS_DIR}/hyperdum_model.json'
with open(model_path, 'w') as f:
    json.dump(model_data, f, indent=2)
print(f"   âœ… Model saved: {model_path}")

# Save results
results = {
    'config': {
        'D': hyperdum.D,
        'levels': hyperdum.levels
    },
    'performance': {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'threshold': float(threshold)
    },
    'timing': {
        'train_time_sec': float(train_time),
        'predict_time_sec': float(predict_time),
        'samples_per_sec': float(len(all_positions) / predict_time)
    },
    'confusion_matrix': cm.tolist()
}

json_path = f'{RESULTS_DIR}/day5_results.json'
with open(json_path, 'w') as f:
    json.dump(results, f, indent=2)
print(f"   âœ… Results JSON: {json_path}")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*70)
print("âœ… DAY 5 COMPLETE")
print("="*70)
print(f"\nKey Results:")
print(f"  ðŸŽ¯ Recall (anomaly detection): {recall*100:.1f}%")
print(f"  ðŸŽ¯ Precision: {precision*100:.1f}%")
print(f"  âš¡ Inference: {predict_time/len(all_positions)*1000:.3f}ms per sample")
print(f"  ðŸ“Š F1-Score: {f1*100:.1f}%")
print(f"\nReady for Day 6: Complete system integration")
print("="*70)