# -*- coding: utf-8 -*-
"""FFNN_GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ij2QIinpqEWsgia7vwvGcF-CcOUQ9Rth
"""

# ============================================================================
# Day 4: Feedforward Neural Network + GPU Acceleration
# MSIM815 Final Project - Bias Correction Layer
# ============================================================================

"""
PURPOSE: Train FFNN to correct systematic EKF bias using GPU
OUTPUTS:
  - Trained FFNN model
  - GPU speedup measurements
  - Corrected position estimates
"""

# ============================================================================
# SETUP
# ============================================================================

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import time
import json
import os

BASE_DIR = '/content/drive/MyDrive/MSIM815'
DATA_DIR = f'{BASE_DIR}/data/ffnn_training'
RESULTS_DIR = f'{BASE_DIR}/results/day4'
MODEL_DIR = f'{BASE_DIR}/models'

os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# Check GPU availability
device_gpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_cpu = torch.device('cpu')

print(f"GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Using device: {device_gpu}")

print("‚úÖ Setup complete")

# ============================================================================
# DEFINE FFNN ARCHITECTURE
# ============================================================================

class BiasCorrector(nn.Module):
    """
    Feedforward Neural Network for EKF bias correction

    Architecture: 3 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 3
    Input: EKF position estimate [x, y, z]
    Output: Correction [Œîx, Œîy, Œîz]
    """

    def __init__(self):
        super(BiasCorrector, self).__init__()

        self.network = nn.Sequential(
            # Layer 1: 3 ‚Üí 64
            nn.Linear(3, 64),
            nn.ReLU(),

            # Layer 2: 64 ‚Üí 32
            nn.Linear(64, 32),
            nn.ReLU(),

            # Layer 3: 32 ‚Üí 16
            nn.Linear(32, 16),
            nn.ReLU(),

            # Layer 4: 16 ‚Üí 3
            nn.Linear(16, 3)
        )

    def forward(self, x):
        return self.network(x)

    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

# ============================================================================
# LOAD DATA
# ============================================================================

print("\nüìÇ Loading training data...")

# Load train/val/test splits
train_ekf = pd.read_csv(f'{DATA_DIR}/train/ekf_outputs.csv').values
train_corr = pd.read_csv(f'{DATA_DIR}/train/corrections.csv').values

val_ekf = pd.read_csv(f'{DATA_DIR}/val/ekf_outputs.csv').values
val_corr = pd.read_csv(f'{DATA_DIR}/val/corrections.csv').values

test_ekf = pd.read_csv(f'{DATA_DIR}/test/ekf_outputs.csv').values
test_corr = pd.read_csv(f'{DATA_DIR}/test/corrections.csv').values

print(f"   Train: {len(train_ekf):,} samples")
print(f"   Val:   {len(val_ekf):,} samples")
print(f"   Test:  {len(test_ekf):,} samples")

# Convert to PyTorch tensors
train_dataset = TensorDataset(
    torch.FloatTensor(train_ekf),
    torch.FloatTensor(train_corr)
)
val_dataset = TensorDataset(
    torch.FloatTensor(val_ekf),
    torch.FloatTensor(val_corr)
)
test_dataset = TensorDataset(
    torch.FloatTensor(test_ekf),
    torch.FloatTensor(test_corr)
)

# Create data loaders
batch_size = 512
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

print(f"   Batch size: {batch_size}")
print(f"   Train batches: {len(train_loader)}")

# ============================================================================
# TRAINING FUNCTION
# ============================================================================

def train_model(model, train_loader, val_loader, device, epochs=50, lr=0.001):
    """
    Train the FFNN model

    Returns:
        model: Trained model
        history: Training history
    """
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    history = {
        'train_loss': [],
        'val_loss': [],
        'epoch_times': []
    }

    print(f"\nüîÑ Training on {device}...")
    print(f"   Epochs: {epochs}")
    print(f"   Learning rate: {lr}")
    print(f"   Parameters: {model.count_parameters():,}")

    for epoch in range(epochs):
        epoch_start = time.time()

        # Training
        model.train()
        train_loss = 0.0

        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # Validation
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()

        val_loss /= len(val_loader)

        epoch_time = time.time() - epoch_start

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['epoch_times'].append(epoch_time)

        if (epoch + 1) % 10 == 0:
            print(f"   Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {train_loss:.6f}, "
                  f"Val Loss: {val_loss:.6f}, "
                  f"Time: {epoch_time:.2f}s")

    return model, history

# ============================================================================
# CPU TRAINING
# ============================================================================

print("\n" + "="*70)
print("CPU TRAINING")
print("="*70)

model_cpu = BiasCorrector()
cpu_start = time.time()
model_cpu, history_cpu = train_model(
    model_cpu, train_loader, val_loader,
    device_cpu, epochs=50, lr=0.001
)
cpu_time = time.time() - cpu_start

print(f"\n‚úÖ CPU training complete: {cpu_time:.2f}s")

# ============================================================================
# GPU TRAINING
# ============================================================================

if torch.cuda.is_available():
    print("\n" + "="*70)
    print("GPU TRAINING")
    print("="*70)

    model_gpu = BiasCorrector()

    # Warmup
    warmup_x, warmup_y = next(iter(train_loader))
    warmup_x, warmup_y = warmup_x.to(device_gpu), warmup_y.to(device_gpu)
    _ = model_gpu(warmup_x)
    torch.cuda.synchronize()

    gpu_start = time.time()
    model_gpu, history_gpu = train_model(
        model_gpu, train_loader, val_loader,
        device_gpu, epochs=50, lr=0.001
    )
    torch.cuda.synchronize()
    gpu_time = time.time() - gpu_start

    speedup = cpu_time / gpu_time

    print(f"\n‚úÖ GPU training complete: {gpu_time:.2f}s")
    print(f"‚ö° Speedup: {speedup:.2f}x")

    # Use GPU model for testing
    model_final = model_gpu
    history_final = history_gpu
else:
    print("\n‚ö†Ô∏è GPU not available, using CPU model")
    model_final = model_cpu
    history_final = history_cpu
    gpu_time = None
    speedup = None

# ============================================================================
# EVALUATE ON TEST SET
# ============================================================================

print("\nüìä Evaluating on test set...")

model_final.eval()
test_predictions = []
test_targets = []

with torch.no_grad():
    for batch_x, batch_y in test_loader:
        batch_x = batch_x.to(device_gpu if torch.cuda.is_available() else device_cpu)
        outputs = model_final(batch_x)
        test_predictions.append(outputs.cpu().numpy())
        test_targets.append(batch_y.numpy())

test_predictions = np.vstack(test_predictions)
test_targets = np.vstack(test_targets)

# Calculate metrics
mse = np.mean((test_predictions - test_targets) ** 2)
mae = np.mean(np.abs(test_predictions - test_targets))
rmse = np.sqrt(mse)

print(f"\n   Test MSE:  {mse:.6f}")
print(f"   Test MAE:  {mae:.6f}")
print(f"   Test RMSE: {rmse:.6f}")

# Calculate position errors
ekf_positions = test_ekf
corrected_positions = ekf_positions + test_predictions
ground_truth = ekf_positions + test_targets

ekf_errors = np.linalg.norm(ekf_positions - ground_truth, axis=1)
corrected_errors = np.linalg.norm(corrected_positions - ground_truth, axis=1)

improvement = (1 - np.mean(corrected_errors) / np.mean(ekf_errors)) * 100

print(f"\n   EKF mean error:       {np.mean(ekf_errors):.4f} m")
print(f"   Corrected mean error: {np.mean(corrected_errors):.4f} m")
print(f"   Improvement:          {improvement:.2f}%")

# ============================================================================
# VISUALIZE RESULTS
# ============================================================================

print("\nüìä Creating visualizations...")

fig = plt.figure(figsize=(16, 10))

# Plot 1: Training curves
ax1 = plt.subplot(2, 3, 1)
epochs = range(1, len(history_final['train_loss']) + 1)
ax1.plot(epochs, history_final['train_loss'], 'b-', linewidth=2, label='Train Loss')
ax1.plot(epochs, history_final['val_loss'], 'r-', linewidth=2, label='Val Loss')
ax1.set_xlabel('Epoch', fontweight='bold')
ax1.set_ylabel('MSE Loss', fontweight='bold')
ax1.set_title('Training Curves', fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: GPU Speedup
ax2 = plt.subplot(2, 3, 2)
if speedup is not None:
    devices = ['CPU', 'GPU']
    times = [cpu_time, gpu_time]
    colors = ['red', 'green']
    bars = ax2.bar(devices, times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
    for bar, t in zip(bars, times):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{t:.1f}s', ha='center', fontweight='bold')
    ax2.set_ylabel('Training Time (s)', fontweight='bold')
    ax2.set_title(f'GPU Speedup: {speedup:.2f}x', fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
else:
    ax2.text(0.5, 0.5, 'GPU not available', ha='center', va='center',
            fontsize=14, transform=ax2.transAxes)
    ax2.axis('off')

# Plot 3: Error improvement
ax3 = plt.subplot(2, 3, 3)
configs = ['EKF Only', 'EKF + FFNN']
errors = [np.mean(ekf_errors), np.mean(corrected_errors)]
colors = ['orange', 'green']
bars = ax3.bar(configs, errors, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
for bar, err in zip(bars, errors):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
            f'{err:.4f}m', ha='center', fontweight='bold')
ax3.set_ylabel('Mean Error (m)', fontweight='bold')
ax3.set_title(f'Bias Correction: {improvement:.1f}% Improvement', fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')

# Plot 4: Error histogram
ax4 = plt.subplot(2, 3, 4)
bins = np.linspace(0, max(ekf_errors.max(), corrected_errors.max()), 50)
ax4.hist(ekf_errors, bins=bins, alpha=0.6, color='orange', label='EKF', density=True)
ax4.hist(corrected_errors, bins=bins, alpha=0.6, color='green', label='EKF+FFNN', density=True)
ax4.set_xlabel('Position Error (m)', fontweight='bold')
ax4.set_ylabel('Density', fontweight='bold')
ax4.set_title('Error Distribution', fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3)

# Plot 5: Correction magnitude
ax5 = plt.subplot(2, 3, 5)
correction_magnitudes = np.linalg.norm(test_predictions, axis=1)
ax5.hist(correction_magnitudes, bins=50, color='blue', alpha=0.7, edgecolor='black')
ax5.set_xlabel('Correction Magnitude (m)', fontweight='bold')
ax5.set_ylabel('Frequency', fontweight='bold')
ax5.set_title('FFNN Correction Distribution', fontweight='bold')
ax5.grid(True, alpha=0.3)
ax5.axvline(np.mean(correction_magnitudes), color='red', linestyle='--',
           linewidth=2, label=f'Mean: {np.mean(correction_magnitudes):.3f}m')
ax5.legend()

# Plot 6: Summary
ax6 = plt.subplot(2, 3, 6)
ax6.axis('off')

summary = f"""
DAY 4 SUMMARY
{'='*45}

FFNN Architecture:
  Layers:        3‚Üí64‚Üí32‚Üí16‚Üí3
  Parameters:    {model_final.count_parameters():,}
  Activation:    ReLU
  Output:        Position correction [Œîx,Œîy,Œîz]

Training:
  Epochs:        50
  Batch size:    512
  Optimizer:     Adam (lr=0.001)
  Train samples: {len(train_ekf):,}
  Val samples:   {len(val_ekf):,}
  Test samples:  {len(test_ekf):,}

Performance:
  CPU time:      {cpu_time:.1f}s
  GPU time:      {gpu_time:.1f if gpu_time else 0:.1f}s
  Speedup:       {speedup:.2f if speedup else 0:.2f}x

Accuracy:
  EKF error:     {np.mean(ekf_errors):.4f} m
  FFNN error:    {np.mean(corrected_errors):.4f} m
  Improvement:   {improvement:.1f}%
  Test RMSE:     {rmse:.6f}
"""

ax6.text(0.1, 0.5, summary, fontsize=10, family='monospace', verticalalignment='center')

plt.suptitle('Day 4: FFNN Bias Correction + GPU Acceleration',
            fontsize=15, fontweight='bold')
plt.tight_layout()

viz_path = f'{RESULTS_DIR}/day4_ffnn_results.png'
plt.savefig(viz_path, dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ Visualization saved: {viz_path}")

# ============================================================================
# SAVE MODEL AND RESULTS
# ============================================================================

print("\nüíæ Saving model and results...")

# Save model
model_path = f'{MODEL_DIR}/ffnn_bias_corrector.pth'
torch.save(model_final.state_dict(), model_path)
print(f"   ‚úÖ Model saved: {model_path}")

# Save corrected outputs
corrected_df = pd.DataFrame(corrected_positions, columns=['x', 'y', 'z'])
corrected_path = f'{RESULTS_DIR}/corrected_positions.csv'
corrected_df.to_csv(corrected_path, index=False)
print(f"   ‚úÖ Corrected positions: {corrected_path}")

# Save results JSON
results = {
    'architecture': {
        'layers': [3, 64, 32, 16, 3],
        'parameters': model_final.count_parameters(),
        'activation': 'ReLU'
    },
    'training': {
        'epochs': 50,
        'batch_size': batch_size,
        'learning_rate': 0.001,
        'optimizer': 'Adam',
        'cpu_time_sec': cpu_time,
        'gpu_time_sec': gpu_time if gpu_time else None,
        'speedup': speedup if speedup else None
    },
    'performance': {
        'test_mse': float(mse),
        'test_mae': float(mae),
        'test_rmse': float(rmse),
        'ekf_mean_error': float(np.mean(ekf_errors)),
        'corrected_mean_error': float(np.mean(corrected_errors)),
        'improvement_pct': float(improvement)
    }
}

json_path = f'{RESULTS_DIR}/day4_results.json'
with open(json_path, 'w') as f:
    json.dump(results, f, indent=2)
print(f"   ‚úÖ Results JSON: {json_path}")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*70)
print("‚úÖ DAY 4 COMPLETE")
print("="*70)
print(f"\nKey Results:")
print(f"  üß† FFNN parameters: {model_final.count_parameters():,}")
print(f"  ‚ö° GPU speedup: {speedup:.2f}x" if speedup else "  ‚ö†Ô∏è No GPU available")
print(f"  üìâ Error reduction: {improvement:.1f}%")
print(f"  üìä {np.mean(ekf_errors):.4f}m ‚Üí {np.mean(corrected_errors):.4f}m")
print(f"\nReady for Day 5: HyperDUM anomaly detection")
print("="*70)